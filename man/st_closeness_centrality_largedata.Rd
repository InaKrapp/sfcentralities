% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/st_network_centrality.R
\name{st_closeness_centrality_largedata}
\alias{st_closeness_centrality_largedata}
\title{A function to find points with minimal sums of distances in large dodgr graphs}
\usage{
st_closeness_centrality_largedata(graph, normalized, chunk_size = 1000)
}
\arguments{
\item{graph}{A dodgr graph}

\item{normalized}{Compute normalized closeness centrality? Expects TRUE or FALSE}

\item{chunk_size}{The number of elements in the graph for which distances should be calculated at once. Set to 1000 by default.}
}
\value{
A vector of closeness values, one for each of the vertices.
}
\description{
This function allows to find points whose sum of distances to other
points is small in large dodgr graphs. It does so by calculating the sum of distances between
points in a batch and all points.
and only keeping the point (or points) with minimal sums before continuing
to the next batch. This allows to find a point with a minimal sum of distances
to other points when the dodgr graph is too large to calculate distances
between all points at once.
Dodgr is well parallelized, so the calculation itself is not usually
the bottleneck. The main reason to calculate distances in batches instead
of all at once is that calculation of distances between n points creates
a result of the dimension n^2, which may be too large to store in memory.
Note that this function therefore only keeps the computed distances to
few points.
}
\examples{
library(dodgr)
# Create graph of hampi, a dataset within the dodgr package, for demonstration
graph <- weight_streetnet(hampi, wt_profile = "foot")
point_candidates <- st_closeness_centrality_largedata(graph, normalized = TRUE)
}
